{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9fa800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def get_data(file=\"data/Train.csv\", fraction=1, drop=[], seed=42):\n",
    "    df = pd.read_csv(file, parse_dates=[\"Date\"],date_format=\"%Y-%m-%d\" )\n",
    "    df[\"Date\"] = df[\"Date\"].values.astype(np.int64) // 10**9\n",
    "    df = df.sample(frac=fraction, random_state=seed)\n",
    "    df = df.drop(drop, axis=1)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec84a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers_sigma(df, column, n_sigma=3):\n",
    "    \"\"\"\n",
    "    Identify outliers using n-sigma rule\n",
    "    Values beyond mean Â± n*std are considered outliers\n",
    "    \"\"\"\n",
    "    mean = df[column].mean()\n",
    "    std = df[column].std()\n",
    "    \n",
    "    lower_bound = mean - (n_sigma * std)\n",
    "    upper_bound = mean + (n_sigma * std)\n",
    "    \n",
    "    # Create boolean mask for outliers\n",
    "    #outliers = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "    \n",
    "    return lower_bound, upper_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe9a6916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "def split(dataframe, target, stratify,seed=42):\n",
    "    target = \"target\"\n",
    "    \n",
    "\n",
    "    y = dataframe[target]\n",
    "    X = dataframe.drop(target, axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed,stratify=stratify )\n",
    "\n",
    "    #df_train = pd.concat((X_train,y_train), axis=1)\n",
    "    return  X_train, X_test, y_train, y_test\n",
    "    #df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0112cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_threshold(dataframe,threshold,drop, excludes):\n",
    "    # features to be encoded\n",
    "\n",
    "\n",
    "    # derive list of columns with more than X % of not null values\n",
    "    columns = {\"column\":[], \"not_null\": [], \"exclude\":[]}\n",
    "    column_list = dataframe.columns.to_list()\n",
    "    for column in column_list:\n",
    "        excl=False\n",
    "        for exclude in excludes:\n",
    "            if exclude in column:\n",
    "                excl = True                \n",
    "        \n",
    "        columns[\"column\"].append(column)\n",
    "        columns[\"not_null\"].append(dataframe[column].count()/dataframe['target'].count()*100)\n",
    "        columns[\"exclude\"].append(excl)\n",
    "\n",
    "    cols=pd.DataFrame(columns)\n",
    "    features_to_use=list(set(cols[(cols[\"not_null\"]>threshold)&(cols[\"exclude\"]==False)][\"column\"])-set([\"target\"])-set(drop))\n",
    "    return features_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccdd4ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, OneHotEncoder, PolynomialFeatures, FunctionTransformer, QuantileTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "import time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "def run_models(model_class, scaler, features_categorical,features_numeric,X_train, y_train, X_test, y_test ):\n",
    "\n",
    "    param_dist = {\n",
    "        'model__max_depth': stats.randint(3, 10),\n",
    "        'model__learning_rate': stats.uniform(0.01, 0.1),\n",
    "        'model__subsample': stats.uniform(0.5, 0.5),\n",
    "        'model__n_estimators':stats.randint(50, 200)\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Show best parameters\n",
    "    #print('Best score:\\n{:.2f}'.format(search_logreg.best_score_))\n",
    "    #print(\"Best parameters:\\n{}\".format(search_logreg.best_params_))\n",
    "    # Save best model (including fitted preprocessing steps) as best_model \n",
    "    # Calculating the accuracy for the LogisticRegression Classifier \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    #print(features_num_key, features_numeric)\n",
    "    start_time = time.time()\n",
    "    encode_location = Pipeline([ \n",
    "        (\"encode\", OneHotEncoder(drop=\"first\"))]) \n",
    "    \n",
    "    scale_impute = Pipeline([   \n",
    "        (\"imputer_null\",KNNImputer(n_neighbors=10, weights=\"distance\")), # deals with null  \n",
    "        (\"imputer_0\",KNNImputer(n_neighbors=10, weights=\"distance\", missing_values=0)), # deals with 0  \n",
    "        (\"scaler\", scaler )\n",
    "        ]) \n",
    "    preprocess = ColumnTransformer([    \n",
    "        (\"scale_impute\", scale_impute, features_numeric),\n",
    "        (\"encode_location\", encode_location, features_categorical)],remainder=\"drop\")\n",
    "    \n",
    "    regression = Pipeline([ \n",
    "        ('preprocess', preprocess), \n",
    "        ('model', model_class)])\n",
    "    \n",
    "    search_reg = RandomizedSearchCV(regression, param_distributions=param_dist, cv=5, scoring='r2', verbose=1, n_jobs=-1, n_iter=10)\n",
    "\n",
    "    search_reg.fit(X_train, y_train)\n",
    "    best_model = search_reg.best_estimator_\n",
    "    pred = best_model.predict(X_test)\n",
    "    #pred = regression.predict(X_test) \n",
    "    '''\n",
    "    coefficients={\"columns\":[],\"numbers\":[]}\n",
    "    if model_name==\"XGBoost\":\n",
    "        coefficients[\"numbers\"] = regression.named_steps[\"model\"][0].feature_importances_\n",
    "        coefficients[\"columns\"] = regression[:-1].get_feature_names_out()\n",
    "        \n",
    "    elif model_name==\"Polynomial Linear Regression\":\n",
    "        coefficients[\"numbers\"] = regression.named_steps[\"model\"][1].coef_\n",
    "        coefficients[\"columns\"] = regression.named_steps[\"model\"][0].get_feature_names_out()\n",
    "  \n",
    "    \n",
    "    coefficients = pd.DataFrame(coefficients)\n",
    "    coefficients[\"numbers\"]=coefficients[\"numbers\"].abs()\n",
    "    coefficients=coefficients.sort_values(by=\"numbers\",ascending=False)\n",
    "    coefficients[\"columns\"]=coefficients[\"columns\"].replace(to_replace=\"scale_impute__\",value=\"\", regex=True).replace(to_replace=\"encode_location__\",value=\"\", regex=True)\n",
    "    coefficients=\", \".join(coefficients[\"columns\"].head(3).to_list())\n",
    "    '''\n",
    "\n",
    "    r2 = r2_score(y_test, pred)*100\n",
    "    rmse =root_mean_squared_error(y_test, pred)\n",
    "    end_time = time.time()\n",
    "    duration = round(end_time-start_time,0)\n",
    "    return r2, rmse, duration#, coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c99086a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'learning_rate' for estimator Pipeline(steps=[('xgbregressor',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, device=None,\n                              early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, feature_weights=None,\n                              gamma=None, grow_policy=None,\n                              importance_type=None,\n                              interaction_constraints=None, learning_rate=None,\n                              max_bin=None, max_cat_threshold=None,\n                              max_cat_to_onehot=None, max_delta_step=None,\n                              max_depth=None, max_leaves=None,\n                              min_child_weight=None, missing=nan,\n                              monotone_constraints=None, multi_strategy=None,\n                              n_estimators=None, n_jobs=None,\n                              num_parallel_tree=None, ...))]). Valid parameters are: ['memory', 'steps', 'transform_input', 'verbose'].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/gunar/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 490, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/home/gunar/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/gunar/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/joblib/parallel.py\", line 607, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/gunar/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/joblib/parallel.py\", line 607, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/gunar/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 147, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/gunar/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 847, in _fit_and_score\n    estimator = estimator.set_params(**clone(parameters, safe=False))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/gunar/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/sklearn/pipeline.py\", line 319, in set_params\n    self._set_params(\"steps\", **kwargs)\n  File \"/home/gunar/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/sklearn/utils/metaestimators.py\", line 69, in _set_params\n    super().set_params(**params)\n  File \"/home/gunar/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/sklearn/base.py\", line 357, in set_params\n    valid_params[key].set_params(**sub_params)\n  File \"/home/gunar/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/sklearn/pipeline.py\", line 319, in set_params\n    self._set_params(\"steps\", **kwargs)\n  File \"/home/gunar/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/sklearn/utils/metaestimators.py\", line 69, in _set_params\n    super().set_params(**params)\n  File \"/home/gunar/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/sklearn/base.py\", line 345, in set_params\n    raise ValueError(\nValueError: Invalid parameter 'learning_rate' for estimator Pipeline(steps=[('xgbregressor',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, device=None,\n                              early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, feature_weights=None,\n                              gamma=None, grow_policy=None,\n                              importance_type=None,\n                              interaction_constraints=None, learning_rate=None,\n                              max_bin=None, max_cat_threshold=None,\n                              max_cat_to_onehot=None, max_delta_step=None,\n                              max_depth=None, max_leaves=None,\n                              min_child_weight=None, missing=nan,\n                              monotone_constraints=None, multi_strategy=None,\n                              n_estimators=None, n_jobs=None,\n                              num_parallel_tree=None, ...))]). Valid parameters are: ['memory', 'steps', 'transform_input', 'verbose'].\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ex_key, ex_val \u001b[38;5;129;01min\u001b[39;00m column_exclusion.items():\n\u001b[32m     46\u001b[39m         features_other = column_threshold(dataframe=df,threshold=threshold,drop=features_categorical, excludes=ex_val)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m         r2, rmse, duration = \u001b[43mrun_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscaler_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mfeatures_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeatures_other\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m         results[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m].append(model_name)\n\u001b[32m     49\u001b[39m         results[\u001b[33m\"\u001b[39m\u001b[33mscaler\u001b[39m\u001b[33m\"\u001b[39m].append(scaler_key)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mrun_models\u001b[39m\u001b[34m(model_class, scaler, features_categorical, features_numeric, X_train, y_train, X_test, y_test)\u001b[39m\n\u001b[32m     51\u001b[39m regression = Pipeline([ \n\u001b[32m     52\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mpreprocess\u001b[39m\u001b[33m'\u001b[39m, preprocess), \n\u001b[32m     53\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m, model_class)])\n\u001b[32m     55\u001b[39m search_reg = RandomizedSearchCV(regression, param_distributions=param_dist, cv=\u001b[32m5\u001b[39m, scoring=\u001b[33m'\u001b[39m\u001b[33mr2\u001b[39m\u001b[33m'\u001b[39m, verbose=\u001b[32m1\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m, n_iter=\u001b[32m10\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43msearch_reg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m best_model = search_reg.best_estimator_\n\u001b[32m     59\u001b[39m pred = best_model.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1992\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1990\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1991\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1992\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1993\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1994\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1995\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1996\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/joblib/parallel.py:1784\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait_retrieval():\n\u001b[32m   1779\u001b[39m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[32m   1780\u001b[39m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[32m   1782\u001b[39m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[32m   1783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborting:\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1785\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1787\u001b[39m     nb_jobs = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/joblib/parallel.py:1859\u001b[39m, in \u001b[36mParallel._raise_error_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1855\u001b[39m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[32m   1856\u001b[39m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[32m   1857\u001b[39m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[43merror_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/joblib/parallel.py:758\u001b[39m, in \u001b[36mBatchCompletionCallBack.get_result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    752\u001b[39m backend = \u001b[38;5;28mself\u001b[39m.parallel._backend\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.supports_retrieve_callback:\n\u001b[32m    755\u001b[39m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[32m    756\u001b[39m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[32m    757\u001b[39m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neue_fische/ds-ml-project-pollution1/.venv/lib/python3.11/site-packages/joblib/parallel.py:773\u001b[39m, in \u001b[36mBatchCompletionCallBack._return_or_raise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status == TASK_ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Invalid parameter 'learning_rate' for estimator Pipeline(steps=[('xgbregressor',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, device=None,\n                              early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, feature_weights=None,\n                              gamma=None, grow_policy=None,\n                              importance_type=None,\n                              interaction_constraints=None, learning_rate=None,\n                              max_bin=None, max_cat_threshold=None,\n                              max_cat_to_onehot=None, max_delta_step=None,\n                              max_depth=None, max_leaves=None,\n                              min_child_weight=None, missing=nan,\n                              monotone_constraints=None, multi_strategy=None,\n                              n_estimators=None, n_jobs=None,\n                              num_parallel_tree=None, ...))]). Valid parameters are: ['memory', 'steps', 'transform_input', 'verbose']."
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress all warnings\n",
    "#from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "fraction=1\n",
    "column_threshold_not_null = [20]\n",
    "column_exclusion ={\"sat geometry\":[\"angle\", \"altitude\"]}\n",
    "scaler = {\"power\":make_pipeline(PowerTransformer())}\n",
    "target_outlier_sigma = 3\n",
    "\n",
    "target = \"target\"\n",
    "drop=[\"Place_ID X Date\",\"target_min\",\"target_max\",\"target_variance\",\"target_count\"]\n",
    "\n",
    "file = \"data/Train.csv\"\n",
    "seed=2\n",
    "features_categorical = [\"Place_ID\"]\n",
    "\n",
    "df = get_data(file=file,fraction=fraction, drop=drop, seed=seed)\n",
    "df[target]=np.log10(df[target])\n",
    "\n",
    "X_train, X_test, y_train, y_test = split(dataframe=df, target=target,stratify=df[features_categorical], seed=seed)\n",
    "df_train = pd.concat((X_train,y_train), axis=1)\n",
    "target_min,target_max=identify_outliers_sigma(df_train, target, target_outlier_sigma)\n",
    "df_train = df_train[(df_train[target]>=target_min)&(df_train[target]<=target_max)]\n",
    "\n",
    "X_train = df_train.drop(target,axis=1)\n",
    "y_train = df_train[target]\n",
    "\n",
    "\n",
    "objective = 'reg:squarederror'\n",
    "models ={\"XGBoost\": make_pipeline(xgb.XGBRegressor())  \n",
    "        #,\"Linear Regression 1\": make_pipeline(LinearRegression())\n",
    "        #,\"Polynomial Linear Regression 2\": make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
    "        #,'Random Forest': make_pipeline(RandomForestRegressor(n_estimators=100,min_impurity_decrease=0.1, random_state=42))\n",
    "        #,'SVR': make_pipeline(SVR())\n",
    "        #,\"Gradient Boost Regressor\" : GradientBoostingRegressor()\n",
    "        #,\"LGBoost\": make_pipeline(lgb.LGBMRegressor())\n",
    "        }\n",
    "\n",
    "results =  {\"model\":[],\"scaler\":[], \"R2\":[], \"RMSE\":[],\"duration\":[],\"not null gt\":[],\"exclude\":[],\"columns\":[]}#, \"coefficients\":[]}\n",
    "\n",
    "for model_name, model_class in models.items():\n",
    "        for scaler_key,scaler_value in scaler.items():\n",
    "                for threshold in column_threshold_not_null:\n",
    "                        for ex_key, ex_val in column_exclusion.items():\n",
    "                                features_other = column_threshold(dataframe=df,threshold=threshold,drop=features_categorical, excludes=ex_val)\n",
    "                                r2, rmse, duration = run_models(model_class,scaler_value,  features_categorical,features_other,X_train, y_train, X_test, y_test )\n",
    "                                results[\"model\"].append(model_name)\n",
    "                                results[\"scaler\"].append(scaler_key)\n",
    "                                results[\"R2\"].append(r2)\n",
    "                                results[\"RMSE\"].append(rmse)\n",
    "                                results[\"duration\"].append(duration)\n",
    "                                results[\"not null gt\"].append(threshold)\n",
    "                                results[\"exclude\"].append(ex_key)\n",
    "                                results[\"columns\"].append(len(features_other)\n",
    "                                                          )\n",
    "\n",
    "#                results[\"coefficients\"].append(coefficients)\n",
    "                \n",
    "results=pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a8dcef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>scaler</th>\n",
       "      <th>R2</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>duration</th>\n",
       "      <th>not null gt</th>\n",
       "      <th>exclude</th>\n",
       "      <th>columns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Regression 1</td>\n",
       "      <td>power</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>20</td>\n",
       "      <td>sat geometry</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>power</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>20</td>\n",
       "      <td>sat geometry</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model scaler    R2  RMSE  duration  not null gt   \n",
       "1  Linear Regression 1  power  71.0   0.0      55.0           20  \\\n",
       "0              XGBoost  power  67.0   0.0      56.0           20   \n",
       "\n",
       "        exclude  columns  \n",
       "1  sat geometry       37  \n",
       "0  sat geometry       37  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 500\n",
    "pd.set_option('display.max_rows', None)\n",
    "#results.to_csv(\"results/results_with_y_outlier.csv\")\n",
    "results.to_csv(\"results/final_results_wo_y_outlier.csv\")\n",
    "results.sort_values(by=[\"R2\",\"RMSE\"], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
